# MÃ©thodologie de Recherche â€” Quantlab V7

Pipeline de recherche, biais identifiÃ©s, failles mÃ©thodologiques et bonnes pratiques pour maintenir la rigueur scientifique.

---

## Pipeline de recherche actuel (V5b)

```
1. Ingestion â†’ 2. Diagnostic 2-phases â†’ 3. Portfolio Markowitz â†’ 4. Stress tests â†’ 5. DÃ©ploiement
```

### Ã‰tape 1 : Ingestion (`data/ingestion.py`)
- **Input** : Binance API (1m candles)
- **Output** : Parquet multi-TF (5m, 15m, 1h, 4h, 1d)
- **Validation** : ContinuitÃ© temporelle, gaps gÃ©rÃ©s

### Ã‰tape 2 : Diagnostic 2-phases (`scripts/diagnostic_v5b.py`)
- **Phase 1** : Quick scan defaults sur holdout (132 combos â†’ ~80 survivants)
  - Backtest rapide avec params par dÃ©faut sur donnÃ©es post-cutoff
  - Filtre : Sharpe > -1.5, min 3 trades
- **Phase 2** : Walk-forward multi-seed sur survivants
  - Optuna TPE + MedianPruner, 30 trials, 3M reoptim, 1Y window
  - 3 seeds par combo, mÃ©diane retenue, std mesurÃ©
  - Holdout validation (cutoff 2025-02-01)
  - Baseline + overlay variants
  - V5b : ATR SL/TP + trailing + breakeven + max hold optimisables
  - Risk grid : flat, r0.5%, r1.0%, r2.0%
  - CorrÃ©lation matrix des survivants STRONG
- **Output** : `portfolio/v5b/results/diagnostic_v5b_{ts}.json` + rapport markdown

### Ã‰tape 3 : Portfolio (`scripts/portfolio_v4b_final.py`)
- **Scope** : Survivants STRONG du diagnostic
- **MÃ©thode** : Markowitz contraint (Ledoit-Wolf), top3_heavy, leverage testing
- **Contraintes** : cap symbol 60%, cap combo 25%, dÃ©duplication corr > 0.85
- **Output** : Allocation optimale, Monte Carlo stress tests

### Ã‰tape 4 : Stress tests
- **Monte Carlo** : 5000 sims bootstrapÃ©es sur returns holdout
- **Projections** : Multi-horizon (3M, 6M, 12M, 24M, 36M)
- **Ruin probability** : P(perte > 50%)
- **Stress months** : pire/meilleur mois, pire trimestre

### Ã‰tape 5 : DÃ©ploiement (future)
- Paper trading 2-4 semaines
- Module live/ (signal_runner, executor, scheduler)
- Monitoring Telegram

> **Note** : La mÃ©ta-optimisation (ancienne Ã©tape 3) a Ã©tÃ© **abandonnÃ©e** aprÃ¨s le test A/B (session 5). Les defaults fixes font mieux.

---

## Biais identifiÃ©s dans le pipeline

### Biais #1 : SÃ©lection post-optimisation (Winner Selection Bias)

**Description** :
- On teste 216 combos en diagnostic
- On sÃ©lectionne les 5 meilleurs
- On les mÃ©ta-optimise
- On les met en portfolio

**ProblÃ¨me** : Les 5 "meilleurs" sont probablement les plus chanceux, pas les meilleurs intrinsÃ¨quement.

**Preuve** : Variance Ã©norme entre runs (mÃªme combo : Sharpe -0.50 Ã  +0.72)

**Impact** : Sur-estimation de la performance future

---

### Biais #2 : Data Snooping cumulÃ©

**Description** :
- MÃªme donnÃ©es utilisÃ©es pour diagnostic, mÃ©ta-opt, portfolio
- Pas de holdout final jamais touchÃ©
- Chaque optimisation "apprend" les spÃ©cificitÃ©s des donnÃ©es

**ProblÃ¨me** : Performance qui va se dÃ©grader en live (overfitting aux donnÃ©es historiques)

**Impact** : Sharpes observÃ©s 0.5-0.8 â†’ Sharpes live probablement 0.2-0.4

---

### Biais #3 : Variance non contrÃ´lÃ©e

**Description** :
- Walk-forward stochastique (pas de seed)
- 1 seul run par Ã©valuation
- On optimise du bruit

**ProblÃ¨me** : Les "meilleurs" paramÃ¨tres sont alÃ©atoires

**Impact** : InstabilitÃ© des rÃ©sultats, non-reproductibilitÃ©

---

### Biais #4 : PondÃ©ration scalaire vs optimisation portefeuille

**Description** :
- sharpe_weighted = poids âˆ Sharpe individuel
- Pas de matrice de covariance
- CorrÃ©lations ignorÃ©es

**ProblÃ¨me** : Diversification apparente mais risque cachÃ©

**Preuve** : Portfolio V2 : 2 stratÃ©gies sur ETH = 65% du poids

**Impact** : Sous-estimation du risque rÃ©el

---

### Biais #5 : Look-ahead implicite

**Description** :
- Diagnostic utilise toute l'histoire disponible
- Meta-opt utilise les mÃªmes donnÃ©es
- Pas de sÃ©paration temporelle stricte

**ProblÃ¨me** : Information du futur "fuite" dans le passÃ©

**Impact** : Performance sur-optimiste

---

## Failles mÃ©thodologiques

### Faille #1 : Pas de baseline simple

**Ce qui manque** :
- Test A/B : meta-opt params vs defaults fixes
- Test random : params alÃ©atoires vs optimaux
- Test simple : buy-and-hold, stratÃ©gies naÃ¯ves

**ConsÃ©quence** : On ne sait pas si la complexitÃ© ajoute de la valeur

---

### Faille #2 : Pas de validation out-of-sample finale

**Ce qui manque** :
- Holdout period (derniÃ¨re 12-24 mois) jamais utilisÃ©e
- Test final unique sur cette pÃ©riode
- Validation de la robustesse temporelle

**ConsÃ©quence** : Pas de preuve de gÃ©nÃ©ralisation

---

### Faille #3 : Single-run evaluation

**Ce qui manque** :
- Multi-seed averaging pour chaque Ã©valuation
- Intervalles de confiance sur les mÃ©triques
- Tests de significativitÃ© statistique

**ConsÃ©quence** : Pas de notion d'incertitude

---

### Faille #4 : Espace de recherche mal dÃ©fini

**ProblÃ¨me** :
- Meta-params discrets mais traitÃ©s comme continus
- Espace petit (~1152 combos) mais explorÃ© alÃ©atoirement
- Pas de garantie de trouver l'optimum global

**ConsÃ©quence** : Optimisation inefficace

---

### Faille #5 : Risk modeling incomplet

**Manque** :
- Margin calls (leverage > 1x)
- Slippage extrÃªme en crise
- CorrÃ©lation en crash (tous les actifs baissent ensemble)
- LiquiditÃ© limitÃ©e

**ConsÃ©quence** : Sous-estimation du risque extrÃªme

---

## Bonnes pratiques Ã  implÃ©menter

### 1. SÃ©paration temporelle stricte

```
Train : [2017-01, 2023-12]  â† Diagnostic + Meta-opt
Valid : [2024-01, 2024-12]  â† Portfolio construction
Test  : [2025-01, 2025-12]  â† Validation finale (jamais touchÃ©e)
```

### 2. Multi-seed systÃ©matique

```python
def robust_walk_forward(config, n_seeds=5):
    results = []
    for seed in range(n_seeds):
        np.random.seed(seed * 42 + 7)
        result = run_walk_forward(config)
        results.append(result)
    
    # Prendre la mÃ©diane, pas le max
    median_metrics = compute_median_metrics(results)
    return median_metrics
```

### 3. Baselines systÃ©matiques

- **Defaults fixes** : reoptim=3M, window=1Y, metric=sharpe, trials=100
- **Random search** : params alÃ©atoires dans l'espace mÃ©ta
- **Buy-and-hold** : performance passive de chaque actif
- **Equal weight portfolio** : benchmark simple

### 4. Tests de significativitÃ©

```python
def test_significance(strategy_a, strategy_b, n_bootstraps=1000):
    # Bootstrap des returns
    diff_samples = []
    for _ in range(n_bootstraps):
        sample_a = bootstrap_returns(strategy_a.returns)
        sample_b = bootstrap_returns(strategy_b.returns)
        diff = sample_a.sharpe - sample_b.sharpe
        diff_samples.append(diff)
    
    p_value = np.mean(np.abs(diff_samples) >= abs(observed_diff))
    return p_value
```

### 5. Portfolio-level optimization

```python
def mean_variance_portfolio(returns, target_sharpe=None):
    # Markowitz avec covariance
    # Contraintes : sum(w) = 1, w >= 0, max par actif/stratÃ©gie
    # Objectif : maximiser Sharpe ou minimiser variance pour target return
```

---

## MÃ©triques de robustesse

### 1. Stability Score
```python
stability = 1 - std(sub_period_sharpes) / mean(sub_period_sharpes)
# > 0.7 = stable, < 0.3 = volatile
```

### 2. Cross-seed consistency
```python
consistency = 1 - std(seed_scores) / mean(seed_scores)
# Mesure la reproductibilitÃ©
```

### 3. Out-of-sample decay
```python
decay = oos_sharpe / is_sharpe
# < 0.7 = overfitting suspect
```

### 4. Correlation stress impact
```python
stress_impact = portfolio_sharpe(correlation=1.0) / portfolio_sharpe(correlation=observed)
# Test la sensibilitÃ© aux corrÃ©lations
```

---

## Processus de validation amÃ©liorÃ©

### Phase 1 : Exploration robuste
- Multi-seed diagnostic
- Intervalles de confiance
- Filtrage par stabilitÃ© (pas seulement par performance)

### Phase 2 : Optimisation contrÃ´lÃ©e
- Baselines obligatoires
- Grid search dÃ©terministe (si espace petit)
- Multi-objectif (Sharpe vs DD)

### Phase 3 : Validation temporelle
- Holdout final
- Test de dÃ©gradation temporelle
- ScÃ©narios de crise (2008, 2020, 2022)

### Phase 4 : Portfolio optimisÃ©
- Covariance-aware allocation
- Contraintes rÃ©alistes
- Stress tests multi-dimensionnels

---

## Checklist mÃ©thodologique

### Avant toute optimisation
- [x] Seeds fixÃ©s partout âœ… (session 4, 12 fÃ©v)
- [x] Baselines dÃ©finis âœ… (test A/B, session 5)
- [x] Holdout period rÃ©servÃ©e âœ… (cutoff 2025-02-01, session 7)
- [ ] Tests d'invariants prÃªts

### Pendant l'optimisation
- [x] Multi-seed averaging âœ… (`run_walk_forward_robust`, 5 seeds)
- [ ] Intervalles de confiance calculÃ©s
- [x] Sur-apprentissage surveillÃ© âœ… (IS vs HO comparison)
- [x] Logs complets âœ… (loguru + JSON reports)

### AprÃ¨s l'optimisation
- [x] Validation sur holdout âœ… (sessions 7-8, 23 combos testÃ©s)
- [ ] Tests de significativitÃ©
- [x] Analyse des erreurs âœ… (2 FAIL identifiÃ©s comme sur-fitting)
- [x] Documentation complÃ¨te âœ… (carnet de bord + knowledge base)

---

## Exemples de piÃ¨ges Ã  Ã©viter

### PiÃ¨ge #1 : "Ã‡a marche sur BTC donc Ã§a marchera sur ETH"
**RÃ©alitÃ©** : CorrÃ©lations â‰  1, rÃ©gimes diffÃ©rents

### PiÃ¨ge #2 : "Sharpe 1.2 = stratÃ©gie gÃ©niale"
**RÃ©alitÃ©** : Sharpe 1.2 sur 3 mois avec 10 trades = chance

### PiÃ¨ge #3 : "Plus de paramÃ¨tres = plus puissant"
**RÃ©alitÃ©** : Plus de paramÃ¨tres = plus d'overfitting

### PiÃ¨ge #4 : "Le backtest est rÃ©aliste"
**RÃ©alitÃ©** : Toujours plus optimiste que la rÃ©alitÃ©

### PiÃ¨ge #5 : "La mÃ©ta-opt a trouvÃ© le vrai optimum"
**RÃ©alitÃ©** : Probablement un optimum local bruitÃ©

---

## Indicateurs d'alerte

### ğŸš© Red flags immÃ©diats
- Sharpe > 1.5 avec < 50 trades
- Profit Factor < 1 avec Sharpe > 0.5
- Variance inter-seeds > 50%
- Performance > 50%/an

### âš ï¸ Yellow flags
- Sharpe 0.8-1.2
- DD > 25%
- Concentration > 40% sur un actif
- CorrÃ©lation portfolio > 0.8

### âœ… Green flags
- Sharpe 0.3-0.8
- DD < 20%
- Multi-seed consistency > 0.8
- Diversification rÃ©elle

---

## Philosophie

**La simplicitÃ© est la sophistication** :
- PrÃ©fÃ©rer Sharpe 0.5 stable que Sharpe 2.0 volatile
- La reproductibilitÃ© > la performance apparente
- Les baselines > la complexitÃ© non justifiÃ©e
- L'incertitude mesurÃ©e > la fausse prÃ©cision

**Rigueur avant tout** :
- Chaque optimisation doit Ãªtre questionnÃ©e
- Chaque rÃ©sultat doit Ãªtre validÃ©
- Chaque hypothÃ¨se doit Ãªtre testÃ©e
- Chaque Ã©chec doit Ãªtre appris

---

## LeÃ§ons apprises du holdout (12 fÃ©vrier 2026)

### LeÃ§on #1 : IS Sharpe nÃ©gatif â‰  mauvaise stratÃ©gie
Les combos avec IS Sharpe faible ou nÃ©gatif performent parfois MIEUX en holdout.
Exemple : ETH/supertrend/4h (IS 0.054 â†’ HO 0.444). Cela indique une stratÃ©gie qui ne sur-fitte pas.

### LeÃ§on #2 : Multi-factor > Single-indicator
Le meilleur multi-factor (HO Sharpe 0.935) bat le meilleur simple (0.444) de 2x.
Les filtres de rÃ©gime (ADX) et de volume sont les amÃ©liorations les plus impactantes.

### LeÃ§on #3 : La variance inter-seeds est le vrai signal
Un HO Sharpe de 0.180 avec std=0.13 est PLUS fiable qu'un HO Sharpe de 0.779 avec std=0.86.
Toujours regarder la stabilitÃ©, pas seulement la performance mÃ©diane.

### LeÃ§on #4 : ETH est le marchÃ© le plus "tradable"
7/11 survivants sont sur ETH. BTC est modÃ©rÃ© (3/11), SOL est difficile (1/11).

### LeÃ§on #5 : 4h est le timeframe optimal
6/11 survivants sur 4h. Bon compromis entre frÃ©quence de signaux et bruit.

---

## Pipeline de validation validÃ© (13 fÃ©vrier 2026)

```
1. Diagnostic V5b (2-pass, pruning, multi-seed 3, risk grid)
   â†’ 132 combos â†’ Phase 1 quick scan â†’ Phase 2 WF multi-seed
2. Holdout temporel (cutoff 2025-02-01)
   â†’ IS walk-forward + HO backtest â†’ STRONG/WEAK/FAIL
3. V5b exits avancÃ©es
   â†’ Trailing stop + breakeven + max holding (optimisables)
4. Risk grid comparison
   â†’ flat vs r0.5% vs r1.0% vs r2.0%
5. CorrÃ©lation matrix
   â†’ DÃ©duplication des combos corrÃ©lÃ©s pour portfolio
6. Portfolio Markowitz contraint
   â†’ Covariance Ledoit-Wolf, hard constraints, Monte Carlo
```

---

## Philosophie

**La simplicitÃ© est la sophistication** :
- PrÃ©fÃ©rer Sharpe 0.5 stable que Sharpe 2.0 volatile
- La reproductibilitÃ© > la performance apparente
- Les baselines > la complexitÃ© non justifiÃ©e
- L'incertitude mesurÃ©e > la fausse prÃ©cision

**Rigueur avant tout** :
- Chaque optimisation doit Ãªtre questionnÃ©e
- Chaque rÃ©sultat doit Ãªtre validÃ©
- Chaque hypothÃ¨se doit Ãªtre testÃ©e
- Chaque Ã©chec doit Ãªtre appris
